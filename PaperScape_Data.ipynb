{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "##All imports I used for the following\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import special\n",
    "import scipy as sp\n",
    "import scipy.stats\n",
    "import pandas as pd\n",
    "import csv\n",
    "import math\n",
    "#I like progressbar, because I am rather impatient\n",
    "import progressbar\n",
    "import copy\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "#this just changes the window size for the figures, it is nice for larger displays\n",
    "plt.rcParams['figure.figsize'] = [12, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Each year is a separate file, so you must concatonate all the filenames to be read\n",
    "def return_filenames():\n",
    "    filenames = []\n",
    "    #likely this is a different directory for whomever \n",
    "    general = 'Data/paperscape-data-master/pscp-' \n",
    "    csv = '.csv'\n",
    "    #normally 1991 - 2018\n",
    "    for i in range(1991, 2018):\n",
    "        filenames.append(general + str(i) + csv)\n",
    "    return filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##These are the dataframes that contain all the csv files\n",
    "def get_df_with_data():\n",
    "    filenames = return_filenames()\n",
    "    nam = ['ID', 'Field', 'Found-Ref', 'Total-Ref', 'Ref-List', 'Authors', 'Title']\n",
    "    #there were some difficulties with the data that all the parameters of the read_csv call deal with\n",
    "    data_list = [pd.read_csv(i, sep=';', names=nam, dtype = str, comment = '#', header=None, index_col = False, quoting=3) for i in filenames]\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Primary approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this section details setting up the citation networks\n",
    "#i.e. finding all papers\n",
    "#putting them in their relevant fields\n",
    "#collecting all citations any particular paper has obtained from other papers within its field\n",
    "\n",
    "#note: I do not actually build true networks-- papers are not connected to one another\n",
    "#this is because we only care about citation accrual, not the actual conectedness of the network\n",
    "#although adding this might not be very difficult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#object representing a paper\n",
    "class paper_details:\n",
    "    def __init__(self, authors, ID, field, total_ref, found_ref, ref_list):\n",
    "        self.authors = authors #all authors of the paper, usually a list\n",
    "        self.ID = ID #unique ID\n",
    "        self.field = field #the field the paper is published in\n",
    "        self.total_ref = total_ref #total number of references the paper makes\n",
    "        self.found_ref = found_ref #actual number of references which could actually be made-- i.e. show up in the network\n",
    "        self.citations = 0 \n",
    "        self.ref_list = ref_list #list of references that it makes, meaning a list of unique IDs\n",
    "        self.order = 0\n",
    "    def cited(self):\n",
    "        self.citations += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Dictionary storing papers by their unique ID\n",
    "def make_data_lookup(data_list):\n",
    "    pbar = progressbar.ProgressBar()\n",
    "    data_lookup = {}\n",
    "    for data in pbar(data_list):\n",
    "        #I always assume a paper is part of the citation network related to the first field listed in the data\n",
    "        a = data.Field.str.split('.', expand = True)\n",
    "        a = a[0].str.split(',', expand = True)\n",
    "        data['Field'] = a[0] #i.e. this\n",
    "        \n",
    "        for index, row in data.iterrows(): \n",
    "            if pd.isnull(row['Authors']): #no authors listed in the data\n",
    "                aut = []\n",
    "            else: \n",
    "                aut = row[\"Authors\"].split(',') #get all unique authors\n",
    "            \n",
    "            if pd.isnull(row['Ref-List']): #if paper doesnt make citations to papers in arXiv database\n",
    "                ref_list = []\n",
    "            else:\n",
    "                ref_list = row[\"Ref-List\"].split(',') #all papers in arXiv database that the paper cites\n",
    "            \n",
    "            #intialize the paper-- make a paper object\n",
    "            data_lookup[row['ID']] = paper_details(aut, str(row['ID']), row['Field'], int(row[\"Total-Ref\"]), int(row[\"Found-Ref\"]), ref_list)\n",
    "\n",
    "    return data_lookup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##build out the citations that any paper obtains\n",
    "def collect_citations(data_lookup, verbose = False):\n",
    "    for key, value in data_lookup.items(): \n",
    "        if value.found_ref != 0:\n",
    "            for c in value.ref_list: #iterate through IDs in the papers reference list\n",
    "                    try: #check if ID is even found\n",
    "                        if data_lookup[c].field == value.field: #check that it has same field \n",
    "                            data_lookup[c].cited()\n",
    "                    except KeyError:\n",
    "                        if verbose:\n",
    "                            print('{} not found in Network'.format(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##make a list of fields--so accessing individual citation networks is possible\n",
    "def all_fields(data_list):\n",
    "    fields = []\n",
    "    for data in data_list:\n",
    "        fields.extend(data.Field)\n",
    "    return list(set(fields))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Assigns all papers to thier relevant field\n",
    "def paper_dic(fields, data_lookup):\n",
    "    dic_by_field = {}\n",
    "    for field in fields:\n",
    "        dic_by_field[field] = []\n",
    "    for key, value in data_lookup.items(): \n",
    "        temp = key.split('/')\n",
    "        value.ID = str(temp[-1])\n",
    "        dic_by_field[value.field].append(value)\n",
    "    return dic_by_field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "#Implement the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (27 of 27) |########################| Elapsed Time: 0:02:09 Time:  0:02:09\n"
     ]
    }
   ],
   "source": [
    "data_list = get_df_with_data()\n",
    "data_lookup = make_data_lookup(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_citations(data_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = all_fields(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_by_field = paper_dic(fields, data_lookup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Secondary approach "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this section also details setting up the citation networks\n",
    "#i.e. finding all papers\n",
    "#putting them in their relevant fields\n",
    "#collecting all citations any particular paper has obtained from other papers within its field\n",
    "#however, in this approach a paper can be in multiple fields\n",
    "#if a paper exists in multiple fields, it is a different object in each field\n",
    "#this means that the \"same\" paper will be different objects and have different number of citations depending on field \n",
    "\n",
    "#note, this approach is not necessarily recommended\n",
    "#confer with Manuel\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Dictionary storing papers by their unique ID\n",
    "def make_data_lookup_secondary(data_list):\n",
    "    pbar = progressbar.ProgressBar()\n",
    "    data_lookup = {}\n",
    "    \n",
    "    for data in pbar(data_list):\n",
    "        for index, row in data.iterrows():\n",
    "            #here is where the function differs from the primary approach\n",
    "            #we instead store all fields the paper was classified as\n",
    "            a = row[\"Field\"].split(',')\n",
    "            field_list = []\n",
    "            for i in a:\n",
    "                temp = i.split('.')\n",
    "                field_list.append(temp[0])\n",
    "            if pd.isnull(row['Authors']):\n",
    "                aut = []\n",
    "            else: \n",
    "                aut = row[\"Authors\"].split(',')\n",
    "            if pd.isnull(row['Ref-List']):\n",
    "                ref_list = []\n",
    "            else:\n",
    "                ref_list = row[\"Ref-List\"].split(',') #all papers in arXiv database that the paper cites\n",
    "            data_lookup[row['ID']] = paper_details(aut, str(row['ID']), field_list, int(row[\"Total-Ref\"]), int(row[\"Found-Ref\"]), ref_list)\n",
    "\n",
    "\n",
    "    return data_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seperate papers by field, i.e. build citation networks\n",
    "def paper_dic_secondary(fields, data_lookup):\n",
    "    dic_by_field = {}\n",
    "    dic_by_field_lookup = {}\n",
    "    for field in fields:\n",
    "        dic_by_field[field] = []\n",
    "        dic_by_field_lookup[field] = {}\n",
    "    for key, value in data_lookup.items(): \n",
    "        temp = key.split('/')\n",
    "        value.ID = str(temp[-1])\n",
    "        for field_ in value.field:\n",
    "            paper = copy.deepcopy(value) #here we create a new paper object for each field a paper is listed as\n",
    "            paper.field = field_\n",
    "            dic_by_field[field_].append(paper) #add each of these paper objects to the relevent field\n",
    "            dic_by_field_lookup[field_][key] = paper\n",
    "    return dic_by_field, dic_by_field_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now that we have created papers for each field\n",
    "#we can give each paper object the relevant number of citations\n",
    "#recall each paper gets citations from papers within its own field\n",
    "def collect_citations_secondary(dic_by_field, dic_by_field_lookup):\n",
    "    pbar = progressbar.ProgressBar()\n",
    "    for key, value in pbar(dic_by_field.items()):\n",
    "        for paper in value:\n",
    "            if paper.found_ref != 0:\n",
    "                for c in paper.ref_list:\n",
    "                        try:\n",
    "                            #this makes sure that papers only cite papers within their field\n",
    "                            dic_by_field_lookup[key][c].cited() \n",
    "                            #if a paper is in the same field then cite, otherwise KeyError\n",
    "                        except KeyError:\n",
    "                            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_lookup = make_data_lookup_secondary(data_list) #remember to remake the data_list\n",
    "\n",
    "# dic_by_field, dic_by_field_lookup = paper_dic_secondary(fields, data_lookup)\n",
    "\n",
    "# collect_citations_secondary(dic_by_field, dic_by_field_lookup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this section I sort the papers by the date of publication\n",
    "#This is necessary to fit the Prices model\n",
    "#Since we inherently assume that index of publication is a primary factor in the number of citations a paper obtains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the data changes shape mid way through 2007\n",
    "#meaning the method for sorting date of publication has to change mid way through the data\n",
    "def sort_by_ID(fields, dic_by_field):\n",
    "    papers_grouped = {}\n",
    "    pbar = progressbar.ProgressBar()\n",
    "    for field in pbar(fields):\n",
    "        \n",
    "        papers_grouped[field] = {}\n",
    "        papers_grouped[field][\"90s\"] = []\n",
    "        papers_grouped[field][\"early_00s\"] = []\n",
    "        papers_grouped[field][\"later_00s\"] = []\n",
    "\n",
    "        for paper in dic_by_field[field]:\n",
    "            #this is very tailored to the data\n",
    "            \n",
    "            #check the first 2 characters of the ID--they give the year\n",
    "            if paper.ID[:2] in ['91', '92', '93', '94', '95', '96', '97', '98', '99']:\n",
    "                #a greater paper ID is correlated to a later date, so store ID as int for comparison\n",
    "                paper.ID = int(paper.ID)\n",
    "                papers_grouped[field][\"90s\"].append(paper)\n",
    "            elif paper.ID[:2] in ['00', '01', '02', '03', '04', '05', '06']:\n",
    "                #I add 1 to the ID, because otherwise the ID is \"0***\"\n",
    "                #meaning when converted to an integer comparisons with \"9**\" are worthless\n",
    "                #despite that \"0**\" dates happen after \"9**\" dates\n",
    "                temp = '1' + paper.ID\n",
    "                paper.ID = int(temp)\n",
    "                papers_grouped[field][\"early_00s\"].append(paper)\n",
    "            elif paper.ID[:2] == '07':\n",
    "                #data changes slightly\n",
    "                #mid way through the 2007, the IDs started to contain decmals\n",
    "                #had to change the method for comparison of IDs\n",
    "                temp = '1' + paper.ID\n",
    "                if float(temp) < 10800.0:\n",
    "                    paper.ID = float(temp)\n",
    "                    papers_grouped[field][\"later_00s\"].append(paper)\n",
    "                else:\n",
    "                    paper.ID = int(temp)\n",
    "                    papers_grouped[field][\"early_00s\"].append(paper)\n",
    "\n",
    "            else:\n",
    "                temp = '1' + str(paper.ID)\n",
    "                paper.ID = float(temp)\n",
    "                papers_grouped[field][\"later_00s\"].append(paper)\n",
    "                \n",
    "    return papers_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (38 of 38) |########################| Elapsed Time: 0:00:01 Time:  0:00:01\n"
     ]
    }
   ],
   "source": [
    "#implement\n",
    "papers_grouped = sort_by_ID(fields, dic_by_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now sort each of the three dataframes independently, then concatonate them\n",
    "def sort_by_date(papers_grouped):\n",
    "    sorted_and_grouped = {}\n",
    "    for field in fields:\n",
    "        sorted_and_grouped[field] = []\n",
    "        #sort the papers by ID, accounting for data changes\n",
    "        papers_grouped[field][\"90s\"].sort(key = lambda paper: paper.ID)\n",
    "        papers_grouped[field][\"early_00s\"].sort(key = lambda paper: paper.ID)\n",
    "        papers_grouped[field][\"later_00s\"].sort(key = lambda paper: paper.ID)\n",
    "        #concatonate\n",
    "        sorted_and_grouped[field].extend(papers_grouped[field][\"90s\"])\n",
    "        sorted_and_grouped[field].extend(papers_grouped[field][\"early_00s\"])\n",
    "        sorted_and_grouped[field].extend(papers_grouped[field][\"later_00s\"])\n",
    "    return sorted_and_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#give each player a unique order\n",
    "#for later analysis\n",
    "def add_order(fields, sorted_and_grouped):\n",
    "    for field in fields:\n",
    "        index = 1\n",
    "        for paper in sorted_and_grouped[field]:\n",
    "            paper.order = index\n",
    "            index+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implement\n",
    "sorted_and_grouped = sort_by_date(papers_grouped)\n",
    "\n",
    "add_order(fields, sorted_and_grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to count the moving average citation count for the jth paper\n",
    "def moving_avg(x, N):\n",
    "    mov_avg = []\n",
    "    N = int(N/2)\n",
    "    print(N)\n",
    "    for i in range(len(x)):\n",
    "        if N+i <= len(x) and i - N >0:\n",
    "            mov_avg.append((1/N)*np.sum(x[i-N:N+i]))\n",
    "        elif N/2+i > len(x):\n",
    "            mov_avg.append((1/(len(x[i-N:])))*np.sum(x[i-N:]))\n",
    "        else:\n",
    "            mov_avg.append((1/(len(x[:N+i])))*np.sum(x[:N+i]))\n",
    "    return mov_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to count the moving average of the predicted g(t) function\n",
    "def gt_avg(y, x, N, n):\n",
    "    mov_avg = []\n",
    "    for i in range(len(x)):\n",
    "        if N+i <= len(x):\n",
    "            mov_avg.append((n/N)*np.trapz(y[i:N+i], x = x[i:N+i]))\n",
    "        else:\n",
    "            mov_avg.append((n/(len(x[i:])))*np.trapz(y[i:N+i], x = x[i:N+i]))\n",
    "    return mov_avg\n",
    "#these 2 functions were very specific for the purposes of displaying some things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Simulations import theoreticalG\n",
    "#here you can plot citation counts of papers based on their index of publication\n",
    "#useful to demonstrate that the number of citations a paper has obtained,\n",
    "#is somewhat related to its date of publication\n",
    "def lil_plottin(sorted_and_grouped):\n",
    "    for field in fields:\n",
    "        \n",
    "        n = len(sorted_and_grouped[field])\n",
    "        factor = 1.0\n",
    "        N = 1000\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        ind = math.floor(n*factor)\n",
    "        x = [i/n for i in range(ind)]\n",
    "        \n",
    "        vals = [i.citations for i in sorted_and_grouped[field][n -ind:]]\n",
    "        print(field)\n",
    "        #uncomment the following lines to display the graphics with \"moving avgs\"\n",
    "        #y = gt_avg(y, x, N, n) #uncomment this line\n",
    "        #vals = moving_avg(vals, N) #uncomment this line \n",
    "        plt.plot(x, vals[:ind], label = 'Citation Count for '+field)\n",
    "        #y = theoreticalG(dic_of_c[field], dic_of_a[field], n, []) #uncomment to plot with g(t)\n",
    "        #plt.plot(x, y[:ind], color = 'red', label = 'g(t)')\n",
    "        plt.xlabel('Scaled Paper Number')\n",
    "        plt.ylabel('Citations')\n",
    "        plt.ylim(bottom = 0, top = max(vals)+5)\n",
    "        plt.legend()\n",
    "        plt.savefig('citation_count/'+field+'_cit_count')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if you plot this with g(t), you need to fit the parameters a and c before hand\n",
    "#lil_plottin(sorted_and_grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collect all the fields with 1000 or more papers\n",
    "def collect_important_fields(fields, sorted_and_grouped):\n",
    "    important_fields = []\n",
    "    for field in fields:\n",
    "        if len(sorted_and_grouped[field]) >= 1000:\n",
    "            important_fields.append(field )\n",
    "    return important_fields\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for the rest of the analysis fields is a smaller subset of all the fields\n",
    "#only fields with more thatn 1000 papers are considered\n",
    "fields = collect_important_fields(fields, sorted_and_grouped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Author and h-index Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this section collects the unique authors \n",
    "#and then find the h-index for every author\n",
    "#and find the h-index distribution for a given m\n",
    "#m is the number of publications an author makes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#object representing author\n",
    "class an_author:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.papers = [] #list of their papers\n",
    "        self.order_list = [] #list of puplication indices\n",
    "        self.order_list2 = []\n",
    "        self.hIndex = 0 #authors true hIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_authors(fields, dic_by_field): #collect all unique authors\n",
    "    pbar = progressbar.ProgressBar()\n",
    "    dic_by_field_authors = {}\n",
    "    all_authors = []\n",
    "    for field in pbar(fields):\n",
    "        dic_by_field_authors[field] = {}\n",
    "        for paper in dic_by_field[field]: #iterate through the papers in the field\n",
    "            all_authors.extend(paper.authors)\n",
    "        for author in list(set(all_authors)): #all unique authors\n",
    "            dic_by_field_authors[field][author] = an_author(author)\n",
    "    return dic_by_field_authors #returns a diction of all unique authors in a field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign the papers to their relevant authors\n",
    "#this function is depreciated\n",
    "#use which_papers_author_for_field_specific instead\n",
    "def which_papers_author(fields, dic_by_field):\n",
    "    pbar = progressbar.ProgressBar()\n",
    "    for field in pbar(fields):\n",
    "        n = len(dic_by_field[field])\n",
    "        print(n)\n",
    "        for paper in dic_by_field[field]:\n",
    "            for aut in paper.authors:\n",
    "                dic_by_field_authors[field][aut].papers.append(paper.citations)\n",
    "                dic_by_field_authors[field][aut].order_list.append(paper.order)\n",
    "                dic_by_field_authors[field][aut].order_list2.append([paper.order/n, paper.citations])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign the papers to their relevant authors\n",
    "def which_papers_author_for_field_specific(fields, dic_by_field):\n",
    "    pbar = progressbar.ProgressBar()\n",
    "    for field in pbar(fields):\n",
    "        for paper in dic_by_field[field]:\n",
    "            for aut in paper.authors:\n",
    "                dic_by_field_authors[field][aut].papers.append(paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mode\n",
    "#assigns the index of publication for every paper\n",
    "#authors order list is filled out with paper index {1,...,n}\n",
    "def one_more_step(fields, dic_by_field_authors):\n",
    "    pbar = progressbar.ProgressBar()\n",
    "    for field in pbar(fields):\n",
    "        n = len(dic_by_field[field])\n",
    "        for aut, author in dic_by_field_authors[field].items():\n",
    "            temp = [paper.field for paper in author.papers]\n",
    "            if temp:\n",
    "                tru_field = mode(temp)\n",
    "                temp2 = [paper.citations for paper in author.papers if paper.field == tru_field ]\n",
    "                temp3 = [paper.order for paper in author.papers if paper.field == tru_field]\n",
    "                temp4 = [paper.order/n for paper in author.papers if paper.field == tru_field]\n",
    "                author.papers.clear()\n",
    "                author.papers.extend(temp2)\n",
    "                author.order_list.extend(temp3)\n",
    "                author.order_list2.extend(temp4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculates the h-index of every author\n",
    "#creates dictionaries with h-indices for all authors for each unique m\n",
    "#where m is the number of papers published by an author\n",
    "\n",
    "#as well as creates a dictionary for frequency of publication\n",
    "#i.e. list of how many papers an author publishes--for all authors\n",
    "def calculate_data_hIndex(fields, dic_by_field_authors):\n",
    "    dic_of_hindex = {}\n",
    "    dic_of_numPapers = {}\n",
    "    pbar = progressbar.ProgressBar()\n",
    "    for field in pbar(fields):\n",
    "        dic_of_hindex[field] = {}\n",
    "        dic_of_numPapers[field] = []\n",
    "        for i in range(2000):#a little gross, but we dont know all the unique number of publications by authors\n",
    "            dic_of_hindex[field][i] = []\n",
    "        dic_of_hindex[field][\"All\"] = []\n",
    "        for author in dic_by_field_authors[field].keys():\n",
    "            dic_by_field_authors[field][author].papers.sort(reverse = True)\n",
    "            \n",
    "            #calculate h-index for each author\n",
    "            if len(dic_by_field_authors[field][author].papers) > 0:\n",
    "                dic_of_numPapers[field].append(len(dic_by_field_authors[field][author].papers)) #gives frequency of all number of papers published by an author\n",
    "                k = 0\n",
    "                while dic_by_field_authors[field][author].papers[k] >= k + 1 and k < len(dic_by_field_authors[field][author].papers)-1:\n",
    "                    k += 1\n",
    "\n",
    "                dic_by_field_authors[field][author].hIndex = k #assign authors h-index\n",
    "                dic_of_hindex[field][len(dic_by_field_authors[field][author].papers)].append(k) #tabulate all h-indices for m publications\n",
    "                dic_of_hindex[field][\"All\"].append(k) #all h-indices independent of publication\n",
    "                \n",
    "    return dic_of_hindex, dic_of_numPapers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the inter-publication times\n",
    "#returns empirical distribution for number of papers published in the interim\n",
    "#between all consecutive pairings of an authors publication indices\n",
    "def paper_pub_freq(fields, dic_by_field_authors):\n",
    "    dist_of_publication = {}\n",
    "    pbar = progressbar.ProgressBar()\n",
    "    for field in pbar(fields):\n",
    "        dist_of_publication[field] = []\n",
    "        for key, aut in dic_by_field_authors[field].items():\n",
    "            aut.order_list.sort()\n",
    "            temp = list(np.diff(aut.order_list))\n",
    "            if temp:\n",
    "                dist_of_publication[field].extend(temp)\n",
    "    return dist_of_publication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate how often frequently authors make m publications\n",
    "#find the empirical distribution for publications made by an author\n",
    "def deterministic_m(fields, dic_of_numPapers):\n",
    "    prob_dic ={}\n",
    "    for field in fields:\n",
    "        temp = list(set(dic_of_numPapers[field]))   \n",
    "        prob_dic[field] = {i : dic_of_numPapers[field].count(i)/len(dic_of_numPapers[field]) for i in temp}\n",
    "    return prob_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculates the empirical total window size of publications\n",
    "#i.e. how many papers between all authors first and last papers\n",
    "def paper_pub_len(fields, dic_by_field_authors):\n",
    "    dist_of_publication = {}\n",
    "    pbar = progressbar.ProgressBar()\n",
    "    for field in pbar(fields):\n",
    "        dist_of_publication[field] = []\n",
    "        for key, aut in dic_by_field_authors[field].items():\n",
    "            aut.order_list.sort()\n",
    "            if len(aut.order_list) > 1:\n",
    "                dist_of_publication[field].append(aut.order_list[-1] - aut.order_list[0])\n",
    "    return dist_of_publication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Implementation of author analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (21 of 21) |########################| Elapsed Time: 0:00:41 Time:  0:00:41\n"
     ]
    }
   ],
   "source": [
    "dic_by_field_authors = find_authors(fields, dic_by_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (21 of 21) |########################| Elapsed Time: 0:00:04 Time:  0:00:04\n"
     ]
    }
   ],
   "source": [
    "which_papers_author_for_field_specific(fields, dic_by_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (21 of 21) |########################| Elapsed Time: 0:00:12 Time:  0:00:12\n"
     ]
    }
   ],
   "source": [
    "one_more_step(fields, dic_by_field_authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (21 of 21) |########################| Elapsed Time: 0:00:05 Time:  0:00:05\n"
     ]
    }
   ],
   "source": [
    "dic_of_hindex, dic_of_numPapers = calculate_data_hIndex(fields, dic_by_field_authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_dic = deterministic_m(fields, dic_of_numPapers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (21 of 21) |########################| Elapsed Time: 0:00:36 Time:  0:00:36\n"
     ]
    }
   ],
   "source": [
    "dist_of_publication = paper_pub_freq(fields, dic_by_field_authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (21 of 21) |########################| Elapsed Time: 0:00:02 Time:  0:00:02\n"
     ]
    }
   ],
   "source": [
    "dist_of_publication_len = paper_pub_len(fields, dic_by_field_authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plots all inter-publication indices\n",
    "#with fitted exponential distribution\n",
    "def plot_pub_dist(fields, dist_of_publication):\n",
    "    \n",
    "    for field in fields:\n",
    "        print(field)\n",
    "        lam = 1/np.mean(dist_of_publication[field])\n",
    "        rv = sp.stats.expon()\n",
    "        fig, ax = plt.subplots()\n",
    "        m = max(dist_of_publication[field])\n",
    "        print(min(dist_of_publication[field]))\n",
    "        x = np.array([i for i in range(m + 1)])\n",
    "        #lam = 1/scale_\n",
    "        print(lam)\n",
    "        ax.plot(x, lam*rv.pdf(lam*x), 'r--', label = 'Exponential pdf with $\\lambda = {0:.3e}$'.format(lam))\n",
    "        ax.hist(dist_of_publication[field], bins = 'auto', density = True, label = 'arXiv Inter-publication Times')\n",
    "        ax.set_ylabel('Frequency')\n",
    "        ax.set_xlabel('Number of Papers Between Publications')\n",
    "        if field == 'hep-ex':\n",
    "            ax.set_xlim(left = 0, right = .01*m)\n",
    "        else:\n",
    "            ax.set_xlim(left = 0, right = .15*m)\n",
    "        plt.legend()\n",
    "        plt.savefig('exp_dist_waiting_time/'+field+'_exp_dist')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plots distribution of first and last publication window indices\n",
    "def plot_pub_dist_len(fields, dist_of_publication):\n",
    "    \n",
    "    for field in fields:\n",
    "        print(field)\n",
    "        fig, ax = plt.subplots()\n",
    "        m = max(dist_of_publication[field])\n",
    "        print(min(dist_of_publication[field]))\n",
    "        x = np.array([i for i in range(m + 1)])\n",
    "   \n",
    "        ax.hist(dist_of_publication[field], bins = 'auto', density = True, label = 'arXiv Publication Range')\n",
    "       \n",
    "        ax.set_ylabel('Frequency')\n",
    "        ax.set_xlabel('Number of Papers Between Publications')\n",
    "\n",
    "        plt.legend()\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_pub_dist(fields, dist_of_publication)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#plot_pub_dist_len(fields, dist_of_publication_len)   #this is the distribution for the range of publications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit a and c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the average number of citations each papers makes\n",
    "#this is somewhat straight forward\n",
    "def find_c_for_field(fields, dic_by_field):\n",
    "    pbar = progressbar.ProgressBar()\n",
    "    dic_of_c = {}\n",
    "    for field in pbar(fields):\n",
    "        temp = []\n",
    "        for paper in dic_by_field[field]:\n",
    "            temp.append(paper.citations)\n",
    "        dic_of_c[field] = np.mean(temp)\n",
    "    return dic_of_c\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (21 of 21) |########################| Elapsed Time: 0:00:00 Time:  0:00:00\n"
     ]
    }
   ],
   "source": [
    "dic_of_c = find_c_for_field(fields, dic_by_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the frequency of papers based on the number of citations\n",
    "#i.e. how many papers have n citations\n",
    "def cit_freq(fields, dic_by_field):\n",
    "    dic_cit_freq ={}\n",
    "    pbar = progressbar.ProgressBar()\n",
    "    for field in pbar(fields):\n",
    "        dic_cit_freq[field] = {}\n",
    "        for paper in dic_by_field[field]:\n",
    "            dic_cit_freq[field][paper.citations] = 0\n",
    "        for paper in dic_by_field[field]:\n",
    "            dic_cit_freq[field][paper.citations] += 1\n",
    "    return dic_cit_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (21 of 21) |########################| Elapsed Time: 0:00:00 Time:  0:00:00\n"
     ]
    }
   ],
   "source": [
    "dic_cit_freq = cit_freq(fields, dic_by_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#was a niave attempt to fit the very non-linear in degree distribution\n",
    "#some of this function is still necessary though\n",
    "\n",
    "def pick_subset(X, y, mse_dic, dic_cit_freq, field, n, middle = True, median = False):\n",
    "    if middle:\n",
    "        pivot = 0.5*(max(X) + min(X))\n",
    "    if median:\n",
    "        pivot = np.median(X)\n",
    "    rang = max(X) - min(X)\n",
    "    for j in [-2.0, -1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5, 2.0]: #permutation of pivot\n",
    "        mse_dic[field][j] = {}\n",
    "        if np.abs(pivot) > np.abs(j): \n",
    "            mid = pivot + j\n",
    "        else:\n",
    "            continue\n",
    "        for i in [(.5, .5), (.5, .1), (.1, .5), (.1, .1), (.1, .2), (.2, .1), (.2, .2), (.1, .3), (.3, .1), (.3, .2), (.2, .3), (.3, .3)]:\n",
    "            y_temp = []\n",
    "            X_temp = []\n",
    "            left = mid - i[0]*rang\n",
    "            right = mid + i[1]*rang\n",
    "            for key, value in dic_cit_freq[field].items():\n",
    "                if value != 0 and key !=0:\n",
    "                    if np.log(key) <= right and np.log(key) >= left: \n",
    "                        y_temp = np.append(y_temp, np.log(float(value)/n))\n",
    "                        X_temp = np.append(X_temp, np.log(key))                         \n",
    "            if len(X_temp) > 0.05*len(X) and len(X_temp) > 2:\n",
    "                reg = LinearRegression().fit(np.array([X_temp]).T, np.array([y_temp]).T)\n",
    "                mse = mean_squared_error(np.array([X_temp]).T, reg.predict(np.array([y_temp]).T))\n",
    "                mse_dic[field][j][i] = {'mse': mse, 'regression' : reg}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regress to find a\n",
    "#This is not used to the regression, but still used to find the in-degree dist\n",
    "#reg_dic contains the in-degree dist\n",
    "def regress_a(fields, dic_cit_freq, dic_by_field):\n",
    "    reg_dic = {}\n",
    "    mse_dic_median = {}\n",
    "    mse_dic_middle = {}\n",
    "    pbar = progressbar.ProgressBar()\n",
    "    for field in pbar(fields): \n",
    "        n = len(dic_by_field[field])\n",
    "        y = np.array([])\n",
    "        X = np.array([])\n",
    "        for key, value in dic_cit_freq[field].items():\n",
    "            if value != 0 and key !=0:\n",
    "                y = np.append(y, np.log(float(value)/n))\n",
    "                X = np.append(X, np.log(key))\n",
    "        mse_dic_median[field] = {}\n",
    "        mse_dic_middle[field] = {}\n",
    "        \n",
    "        pick_subset(X, y, mse_dic_median, dic_cit_freq, field, n, middle = False, median = True) #median\n",
    "        pick_subset(X, y, mse_dic_middle, dic_cit_freq, field, n) #middle\n",
    "        \n",
    "        reg = LinearRegression().fit(np.array([X]).T, np.array([y]).T) #regular regression\n",
    "        reg_dic[field] = {'y' : y, 'X': X, 'regression': reg}\n",
    "        \n",
    "    return reg_dic, mse_dic_median, mse_dic_middle\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (21 of 21) |########################| Elapsed Time: 0:00:08 Time:  0:00:08\n"
     ]
    }
   ],
   "source": [
    "#this is used to find the in-degree distribution\n",
    "reg_dic, mse_dic_median, mse_dic_middle = regress_a(fields, dic_cit_freq, dic_by_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#another method to attempt to fit the in degree distribution\n",
    "#also unused\n",
    "\n",
    "\n",
    "# def find_min_mse(fields, mse_dic, reg_dic):\n",
    "#     min_mse = {}\n",
    "#     for field in fields:\n",
    "#         min_mse[field] = {'min_mse':100000, \"regression\": reg_dic[field]['regression']}\n",
    "#         y = reg_dic[field]['regression'].predict(np.array([reg_dic[field]['X']]).T)\n",
    "#         #y = y.flatten()\n",
    "#         for key, value in mse_dic[field].items():\n",
    "#             for key, value in value.items():\n",
    "#                 if value['regression'].coef_ < -2.5 and value['mse'] < min_mse[field]['min_mse']:\n",
    "#                     min_mse[field]['min_mse'] = value['mse']\n",
    "#                     min_mse[field]['regression'] = value['regression']\n",
    "#     return min_mse \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#another attempt to fit in degree dist\n",
    "#unused\n",
    "\n",
    "\n",
    "# def plot_mse_reg(fields, mse_dic, reg_dic):\n",
    "#     for field in fields:\n",
    "#         print(field)\n",
    "#         y = reg_dic[field]['regression'].predict(np.array([reg_dic[field]['X']]).T)\n",
    "#         #y = y.flatten()\n",
    "#         fig, ax = plt.subplots()\n",
    "#         ax.scatter(reg_dic[field]['X'], reg_dic[field]['y'])\n",
    "#         for key, value in mse_dic[field].items():\n",
    "#             for key, value in value.items():\n",
    "#                 if value['regression'].coef_ < -2.0:\n",
    "#                     y = value['regression'].predict(np.array([reg_dic[field]['X']]).T)\n",
    "#                     ax.plot(reg_dic[field]['X'], y)\n",
    "#         ax.plot(reg_dic[field]['X'], y, color = 'red')\n",
    "        \n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plots the in degree distribution\n",
    "#unused\n",
    "\n",
    "\n",
    "# def plot_reg(fields, reg_dic):\n",
    "#     for field in fields:\n",
    "#         print(field)\n",
    "#         x = np.array([np.linspace(0, max(reg_dic[field]['X']))])\n",
    "#         y = reg_dic[field]['regression'].predict(np.array([reg_dic[field]['X']]).T)\n",
    "        \n",
    "#         x = x.flatten()\n",
    "#         #y = y.flatten()\n",
    "#         print(y.shape)\n",
    "        \n",
    "#         fig, ax = plt.subplots()\n",
    "#         ax.scatter(reg_dic[field]['X'], reg_dic[field]['y'])\n",
    "#         ax.plot(reg_dic[field]['X'], y, color = 'red')\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function definition is misleading\n",
    "#This is used to plot just the indegree distribution, no fit\n",
    "def plot_min_mse(fields, reg_dic):\n",
    "    for field in fields:\n",
    "        print(field)\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.scatter(reg_dic[field]['X'], reg_dic[field]['y']) \n",
    " \n",
    "        ax.set_xlabel('log(k)')\n",
    "        ax.set_ylabel('log(d(k))')\n",
    "\n",
    "        plt.savefig('indegree_dist/'+field+'power_law')\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#After fitting the data, this function returns a dictionary of fitted a's\n",
    "#where the keys are fields\n",
    "def find_a(fields, reg_dic, dic_of_c, tails = False):\n",
    "    dic_of_a = {}\n",
    "    if tails:\n",
    "        for field in fields:\n",
    "            dic_of_a[field] = (dic_of_c[field]*((1- reg_dic[field]['regression'].coef_) -2)).flatten().item()\n",
    "    else:\n",
    "        for field in fields:\n",
    "            dic_of_a[field] = (-dic_of_c[field]*(reg_dic[field]['regression'].coef_ +2)).flatten().item()\n",
    "\n",
    "    return dic_of_a\n",
    "        \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is used to pick the data for the tail distribution 1.5 std from the mean\n",
    "def subset_tailored_mean_and_variance(X, y):\n",
    "    mean = np.mean(X)\n",
    "    std = np.std(X)\n",
    "    X_sub = X[(X < mean+1.5*std) & (X>mean-1.5*std)]\n",
    "    y_sub = y[(X < mean+1.5*std) & (X>mean-1.5*std)]\n",
    "    return(X_sub, y_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this was a niave way to pick data from the tail dist, \n",
    "#unused\n",
    "\n",
    "# def pick_subset_tailored(X, y, mse_dic, dic_cit_freq, field, n, middle = False, median = True):\n",
    "#     if middle:\n",
    "#         pivot = 0.5*(max(X) + min(X))\n",
    "#     if median:\n",
    "#         pivot = np.median(X)\n",
    "#     rang = max(X) - min(X)\n",
    "#     for j in [-2.0, -1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5, 2.0]: #permutation of pivot\n",
    "#         mse_dic[field][j] = {}\n",
    "#         if np.abs(pivot) > np.abs(j): \n",
    "#             mid = pivot + j\n",
    "#         else:\n",
    "#             continue\n",
    "#         for i in [(.1, .1), (.1, .2), (.2, .1), (.2, .2), (.1, .3), (.3, .1), (.3, .2), (.2, .3), (.3, .3), (.1, .4), (.4, .1),(.4, .2),(.2, .4), (.4, .3), (.3, .4), (.4,.4)]:\n",
    "#             left = mid - i[0]*rang\n",
    "#             right = mid + i[1]*rang\n",
    "#             temp = np.array([])\n",
    "#             y_temp = np.array([])\n",
    "#             X_temp= np.array([])\n",
    "            \n",
    "#             for key, value in dic_cit_freq[field].items():\n",
    "#                 if value != 0 and key !=0:\n",
    "#                     if np.log(key) <= right and np.log(key) >= left: \n",
    "#                         temp = np.append(temp, float(value)/n)\n",
    "#                         X_temp = np.append(X_temp, key)\n",
    "#             if len(X_temp)>0:\n",
    "#                 X_temp, temp = zip(*sorted(zip(X_temp,temp), key=lambda pair: pair[0]))\n",
    "            \n",
    "#                 for i in range(len(X_temp)):\n",
    "#                     y_temp = np.append(y_temp, np.sum(temp[i:]))\n",
    "\n",
    "#                 y_temp = np.log(y_temp)\n",
    "#                 X_temp = np.log(X_temp)\n",
    "\n",
    "#                 if len(X_temp) > 0.5*len(X) and len(X_temp) > 2:\n",
    "#                     reg = LinearRegression().fit(np.array([X_temp]).T, np.array([y_temp]).T)\n",
    "#                     mse = mean_squared_error(np.array([X_temp]).T, reg.predict(np.array([y_temp]).T))\n",
    "#                     mse_dic[field][j][i] = {'mse': mse, 'regression' : reg}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot for tail distribution\n",
    "def plot_tails(fields, reg_dic, min_mse_tails):\n",
    "    plt.rcdefaults()\n",
    "    for field in fields:\n",
    "        print(field)\n",
    "        y = reg_dic[field]['regression'].predict(np.array([reg_dic[field]['X']]).T)\n",
    "        y2 = min_mse_tails[field]['regression'].predict(np.array([reg_dic[field]['X']]).T)\n",
    "        X, Y = subset_tailored_mean_and_variance(reg_dic[field]['X'], reg_dic[field]['y'])\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.scatter(reg_dic[field]['X'], reg_dic[field]['y'], label = 'Entire Tail Distribution')\n",
    "        ax.scatter(X, Y, color = 'black', label = '$\\mu \\pm 1.5 \\sigma$')\n",
    "        #ax.plot(reg_dic[field]['X'], y, color = 'red')\n",
    "        ax.plot(reg_dic[field]['X'], y2, color = 'red', label = 'Linear Fit')\n",
    "        ax.set_xlabel('log(k)')\n",
    "        ax.set_ylabel(r'$log \\left( \\sum_{i\\geq k} d(i)\\right)$')\n",
    "        #ax.set_title('{} Tail Indegree Distribution'.format(field))\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('tail_deg/'+field+'tail_dist')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method for both fitting and creating the tail distribution#\n",
    "def tails_reg(fields, dic_cit_freq, dic_by_field, pick_subset = False, mean_subset = True):\n",
    "    reg_dic = {}\n",
    "    mse_dic_median = {}\n",
    "    pbar = progressbar.ProgressBar()\n",
    "    for field in pbar(fields): \n",
    "        n = len(dic_by_field[field])\n",
    "        temp = np.array([])\n",
    "        y = np.array([])\n",
    "        X = np.array([])\n",
    "        #here is the implementation creating the tail distribution\n",
    "        for key, value in dic_cit_freq[field].items():\n",
    "            if value != 0 and key !=0:\n",
    "                temp = np.append(temp, float(value)/n)\n",
    "                X = np.append(X, key)\n",
    "        X, temp = zip(*sorted(zip(X,temp), key=lambda pair: pair[0]))\n",
    "        #tail dist, meaning the sum of data greater than i\n",
    "        for i in range(len(X)):\n",
    "            y = np.append(y, np.sum(temp[i:]))\n",
    "        #take the log \n",
    "        y = np.log(y)\n",
    "        X = np.log(X)\n",
    "        #now fit the data\n",
    "        reg = LinearRegression().fit(np.array([X]).T, np.array([y]).T) #regular regression\n",
    "        reg_dic[field] = {'y' : y, 'X': X, 'regression': reg}\n",
    "        mse_dic_median[field] = {}\n",
    "        if pick_subset:\n",
    "            pick_subset_tailored(X, y, mse_dic_median, dic_cit_freq, field, n)\n",
    "        #this is the method implemented\n",
    "        #1.5 std from the mean\n",
    "        if mean_subset:\n",
    "            X, y = subset_tailored_mean_and_variance(X, y)\n",
    "            reg = LinearRegression().fit(np.array([X]).T, np.array([y]).T) #regular regression\n",
    "            mse_dic_median[field] = {'y' : y, 'X': X, 'regression': reg}\n",
    "    return reg_dic, mse_dic_median\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#picking up subsets of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (21 of 21) |########################| Elapsed Time: 0:00:00 Time:  0:00:00\n"
     ]
    }
   ],
   "source": [
    "#find and fit the tail distribution\n",
    "reg_dic_tails, mean_std_tails = tails_reg(fields, dic_cit_freq, dic_by_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this was previously used to find the fit that minimized the mse of the fit\n",
    "#unused\n",
    "def find_min_mse_tailored(fields, mse_dic, reg_dic):\n",
    "    min_mse = {}\n",
    "    for field in fields:\n",
    "        min_mse[field] = {'min_mse':100000, \"regression\": reg_dic[field]['regression']}\n",
    "        y = reg_dic[field]['regression'].predict(np.array([reg_dic[field]['X']]).T)\n",
    "        #y = y.flatten()\n",
    "        for key, value in mse_dic[field].items():\n",
    "            for key, value in value.items():\n",
    "                if value['mse'] < min_mse[field]['min_mse']:\n",
    "                    min_mse[field]['min_mse'] = value['mse']\n",
    "                    min_mse[field]['regression'] = value['regression']\n",
    "    return min_mse \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_tails(fields, reg_dic_tails, mean_std_tails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_min_mse(fields, reg_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set a to be the a from the 1.5std from mean fit of tail dist\n",
    "dic_of_a_from_tails_mean_std = find_a(fields, mean_std_tails, dic_of_c, tails = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is the method to calculate a from tail distribution\n",
    "def calc_a(reg, c):\n",
    "    a = (-c*(reg.coef_ + 2)).flatten().item()\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this was another niave implentation to attempt to fit a\n",
    "#unused\n",
    "\n",
    "\n",
    "# def bootstrap_a(fields, dic_by_field, dic_cit_freq, dic_of_c, N=1000):\n",
    "\n",
    "#     dic_of_a_boot = {}\n",
    "#     pbar = progressbar.ProgressBar()\n",
    "#     for field in pbar(fields): \n",
    "#         n = len(dic_by_field[field])\n",
    "#         y = np.array([])\n",
    "#         X = np.array([])\n",
    "#         #x_y_pairs = np.array([[0, 0]])\n",
    "#         for key, value in dic_cit_freq[field].items():\n",
    "#             if value != 0 and key !=0:\n",
    "#                 y = np.append(y, np.log(float(value)/n))\n",
    "#                 X = np.append(X, np.log(key))\n",
    "#                 #x_y_pairs = np.concatenate((x_y_pairs, np.array([[np.log(key), np.log(float(value)/n)]])), axis = 0)\n",
    "#         reg = LinearRegression().fit(np.array([X]).T, np.array([y]).T)\n",
    "#         dic_of_a_boot[field] = 2*calc_a(reg, dic_of_c[field])\n",
    "#         a_hat = 0\n",
    "#         for i in range(N):\n",
    "#             resample = np.random.choice(len(X), len(X))\n",
    "#             reg = LinearRegression().fit(np.array([X[resample]]).T, np.array([y[resample]]).T)\n",
    "#             a_hat += calc_a(reg, dic_of_c[field])\n",
    "#         dic_of_a_boot[field] += (-a_hat/N)\n",
    "#     return dic_of_a_boot\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theoretical and comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports from the other document I wrote\n",
    "from Simulations import p_k_Beta, hDist_fromBeta_equals_k, g_inverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#used to find the total h-index probability distribution and the h-index distribution for a given m\n",
    "#where m is number of papers an author has\n",
    "#the distribution of m is empirical\n",
    "#dist_dic is the total h-index distribtuion\n",
    "#conditional_h is the h-index distribution for all possible h-index values given a particular m\n",
    "def hDist(data, data2, fields, prob_dic, c, a):\n",
    "    dist_dic = {}\n",
    "    pbar = progressbar.ProgressBar()\n",
    "    conditional_h = {}\n",
    "    for field in pbar(fields):\n",
    "        dist_dic[field] = []\n",
    "        conditional_h[field] = []\n",
    "        temp = sorted(list(set(data[field])))\n",
    "        temp2 = sorted(list(set(data2[field][\"All\"])))\n",
    "        for k in temp2:#loop over all possible h-index values in the data\n",
    "            sum_over_m = 0\n",
    "            temp3 = []\n",
    "            for m in temp:#now all the possible number of papers an author could have published\n",
    "                h = hDist_fromBeta_equals_k(len(data[field]), k, m, c[field], a[field])\n",
    "                if h < 1e-9 or np.isnan(h):#this is to deal with machine error and problems from really small numbers\n",
    "                    h = 0.0\n",
    "                temp3.append(h)\n",
    "                sum_over_m += h*prob_dic[field][m]\n",
    "            conditional_h[field].append(temp3)\n",
    "            dist_dic[field].append(sum_over_m)\n",
    "            \n",
    "    return dist_dic, conditional_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set dic_of_a to be the dictionary from the values fit earlier\n",
    "dic_of_a = dic_of_a_from_tails_mean_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (21 of 21) |########################| Elapsed Time: 0:00:23 Time:  0:00:23\n"
     ]
    }
   ],
   "source": [
    "hindex_dist_theoretical, conditional_h = hDist(dic_of_numPapers, dic_of_hindex, fields, prob_dic, dic_of_c, dic_of_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function is for the error bars when plotting the h-index distribution for all possible m\n",
    "def suplement(field, prob_dic, conditional_h, dic_of_hindex, dic_of_numPapers):\n",
    "    err_bars = []\n",
    "    l_inverse = 1/len(list(set(dic_of_numPapers[field])))\n",
    "    X = np.array(sorted([[item, key] for key, item in prob_dic[field].items()], key = lambda x: x[1]))[:,0]\n",
    "    V = np.array([[X[i]*(1-X[i]) if j == i else  -X[i]*X[j] for j in range(len(X))] for i in range(len(X))])\n",
    "    for k in range(len(list(set(dic_of_hindex[field][\"All\"])))):\n",
    "        a_k = np.array(conditional_h[field][k])\n",
    "        left, right = sp.stats.norm.interval(0.95)\n",
    "        err_bars.append([np.dot(a_k, X)- left*np.sqrt(l_inverse*np.dot(np.dot(a_k, V), a_k.T)) ,np.dot(a_k, X)+right*np.sqrt(l_inverse*np.dot(np.dot(a_k, V), a_k.T))])\n",
    "    return err_bars  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot h-index distribution of all possible m\n",
    "def plot_for_all_m(dic_of_hindex, hindex_dist_theoretical):\n",
    "    y = {}\n",
    "    N_dic = {}\n",
    "    for field in fields:\n",
    "        N = len(dic_of_hindex[field][\"All\"]) #number of authors in the field\n",
    "        y[field] = np.multiply(N,hindex_dist_theoretical[field]) \n",
    "        N_dic[field] = N\n",
    "    for field in fields:\n",
    "        print(field)\n",
    "        fig, ax = plt.subplots()\n",
    "        x = [i for i in range(len(hindex_dist_theoretical[field]))]\n",
    "        bins_ = [i for i in range(max(list(set(dic_of_hindex[field]['All'])))+1)]\n",
    "        ax.plot(x, hindex_dist_theoretical[field], 'o', color= 'red', label = 'Predicted h-index')\n",
    "        e_bars = np.array(suplement(field, prob_dic, conditional_h, dic_of_hindex, dic_of_numPapers))\n",
    "        ax.errorbar(x, hindex_dist_theoretical[field], yerr = e_bars.T, fmt='none', color = 'red', elinewidth = 1, label = '95% Confidence Interval')\n",
    "        ax.hist(dic_of_hindex[field][\"All\"], bins=bins_, label = 'arXiv Found h-index', align = 'left', density = True)\n",
    "        #ax.set_title('h-index Distribution for {}'.format(field))\n",
    "        ax.set_ylabel('$P(h-index = k)$')\n",
    "        ax.set_xlabel('h-index')\n",
    "        ax.set_xticks(bins_)\n",
    "        ax.set_ylim(bottom = 0)\n",
    "        ax.set_xlim(left = -0.5, right = min(.4*len(bins_), 20))\n",
    "        plt.legend()\n",
    "        plt.savefig('all_m/'+field+'all_m_hindex')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot distributions for all m\n",
    "#plot_for_all_m(dic_of_hindex, hindex_dist_theoretical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Simulations import p_k_Beta, hDist_fromBeta, g_inverse\n",
    "#another function to find the probability distribution for the h-index given an m\n",
    "#this function is also used to select some of the possible m we want to display across all\n",
    "#fields, this was because we wanted to be able to compare across fields\n",
    "#returns a dictionary with h-index distribution of all the m's that are selected\n",
    "#all the m's\n",
    "#and a dictionary of expected values for the chosen m's\n",
    "def p_k_given_m(dic_of_hindex, fields, c, a):\n",
    "    dic_of_dist_for_m = {}\n",
    "    pbar = progressbar.ProgressBar()\n",
    "    m_all_fields = []\n",
    "    ev_dic = {}\n",
    "    samp_all_fields = {}\n",
    "    index = 0\n",
    "    for field in fields:\n",
    "        #find commonality among m's for different fields\n",
    "        all_m = list(dic_of_hindex[field].keys())\n",
    "        if index == 0:\n",
    "            m_all_fields.extend(all_m[:-1])\n",
    "        else:\n",
    "            m_all_fields = list(set(m_all_fields) & set(all_m[:-1]))\n",
    "        index+=1\n",
    "        samp_all_fields[field] = []\n",
    "    samp = m_all_fields \n",
    "    for field in pbar(fields):\n",
    "        dic_of_dist_for_m[field] = {}\n",
    "        ev_dic[field] = {}\n",
    "        temp = []\n",
    "        for i in samp:\n",
    "            if len(dic_of_hindex[field][i]) > 50: #make sure the sample size is large enough\n",
    "                    temp.append(i)\n",
    "        samp_all_fields[field].extend(temp) \n",
    "        if temp:\n",
    "            for m in temp:\n",
    "                dic_of_dist_for_m[field][m] = hDist_fromBeta(100, int(m), c[field], a[field])\n",
    "                ev_dic[field][m] = 0\n",
    "                for k in range(1, m+1): #expected h-index also\n",
    "                    ev_dic[field][m] += p_k_Beta(g_inverse(k, c[field], a[field]), k, m)\n",
    "    return dic_of_dist_for_m, samp_all_fields, ev_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (21 of 21) |########################| Elapsed Time: 0:00:04 Time:  0:00:04\n"
     ]
    }
   ],
   "source": [
    "m_dist,samp,ev_dic = p_k_given_m(dic_of_hindex, fields, dic_of_c, dic_of_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot h-index distribution for all the chosen m--these m are chosen to be similar among fields\n",
    "#for analysis purposesd\n",
    "def plot_for_given_m(fields, m_dist, dic_of_hindex, ev_dic=ev_dic, fake=False, num = 100000, sample = samp, mean = True):\n",
    "    for field in fields:\n",
    "        if fake:\n",
    "            temp = sample[fiemld]\n",
    "        else:\n",
    "            temp = list(m_dist[field].keys())\n",
    "        for m in temp:\n",
    "            if fake:\n",
    "                N = int(num*prob_dic[field][m])\n",
    "            else:\n",
    "                N = len(dic_of_hindex[field][m])\n",
    "            y =  m_dist[field][m]\n",
    "            x = [i for i in range(0, m+1)]\n",
    "            fig, ax = plt.subplots()\n",
    "            print(N)\n",
    "            print(field)\n",
    "            print(m)\n",
    "            bins_ = [i for i in range(max(list(set(dic_of_hindex[field][m])))+1)]\n",
    "            if fake:\n",
    "                ax.hist(dic_of_hindex[field][m], bins=bins_, label = 'arXiv Synthetic h-indices' , density = True, align = 'left')\n",
    "            else:\n",
    "                ax.hist(dic_of_hindex[field][m], bins=bins_, label = 'arXiv Found h-indices' , density = True, align = 'left')\n",
    "            ax.plot(x[:len(bins_)+1], y[:len(bins_)+1], 'o',  markersize=8 ,color = 'red', label = 'Predicted h-index')\n",
    "            if mean:\n",
    "                #also includes the mean plotted as vertical line\n",
    "                y_ev = np.linspace(0, max(y), num = 10)\n",
    "                x_ev = np.array([ev_dic[field][m] for i in range(10)])\n",
    "                x_avg = np.array([np.mean(dic_of_hindex[field][m]) for i in range(10)])\n",
    "                ax.plot(x_ev, y_ev, color = 'red', label = 'Expected h-index')\n",
    "                ax.plot(x_avg, y_ev, color = 'blue', label = 'Average h-index')\n",
    "#extra legend titles\n",
    "#             if fake:\n",
    "#                 ax.set_title('{} Synthetic Authors all with m = {} Papers'.format(N, m), fontsize = 10)\n",
    "#             else:\n",
    "#                 ax.set_title('{} Authors all with m = {} Papers'.format(N, m), fontsize = 10)\n",
    "            ax.set_ylabel('$P(h-index = k | m = {})$'.format(m))\n",
    "            ax.set_xlabel('h-index')\n",
    "            ax.set_xticks(bins_)\n",
    "            plt.legend(title = '{} Authors'.format(N))\n",
    "            if fake:\n",
    "                plt.savefig('fake_h-index_given_m/'+str(m)+field+'h-index')\n",
    "            else:\n",
    "                plt.savefig('h-index_given_m/'+str(m)+field+'h-index')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plot_for_given_m(fields, m_dist, dic_of_hindex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proof Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Simulations import p_k_Beta, g_inverse\n",
    "def p_k(c, a, m, k): #find probability h-index = k\n",
    "    return p_k_Beta(g_inverse(k, c, a), k, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scatter plots of expected h-index and true average h-index for the same m\n",
    "#ev is one axis and avg is other axis\n",
    "def EV_vs_avgH(fields, dic_of_numPapers, dic_of_hindex, dic_of_c, dic_of_a):\n",
    "    for field in fields:\n",
    "        all_m = list(set(dic_of_numPapers[field]))\n",
    "        avg_h = []\n",
    "        EV = []\n",
    "        for m in all_m:\n",
    "            temp = 0\n",
    "            for k in range(1, m+1):\n",
    "                temp += p_k(dic_of_c[field], dic_of_a[field], m, k)\n",
    "            EV.append(temp)\n",
    "            avg_h.append(np.mean(dic_of_hindex[field][m]))\n",
    "        print(field)\n",
    "        plt.scatter(avg_h, EV)\n",
    "        lim1 = max(avg_h)\n",
    "        lim2 = max(EV)\n",
    "        plt.xlim(0, max(lim1, lim2))\n",
    "        plt.ylim(0, max(lim1, lim2))\n",
    "        plt.gca().set_aspect('equal', adjustable='box')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EV_vs_avgH(fields, dic_of_numPapers, dic_of_hindex, dic_of_c, dic_of_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple plots of expected h-index and average h-index\n",
    "#m is on the x axis\n",
    "def plot_for_m(fields, dic_of_numPapers, dic_of_hindex, dic_of_c, dic_of_a):\n",
    "    for field in fields:\n",
    "        all_m = list(set(dic_of_numPapers[field]))\n",
    "        avg_h = []\n",
    "        EV = []\n",
    "        for m in all_m:\n",
    "            temp = 0\n",
    "            for k in range(1, m+1):\n",
    "                temp += p_k(dic_of_c[field], dic_of_a[field], m, k)\n",
    "            EV.append(temp)\n",
    "            avg_h.append(np.mean(dic_of_hindex[field][m]))\n",
    "        print(field)\n",
    "        \n",
    "        lim1 = max(avg_h)\n",
    "        lim2 = max(EV)\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.scatter(all_m, EV)\n",
    "        ax.scatter(all_m, avg_h)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_for_m(fields, dic_of_numPapers, dic_of_hindex, dic_of_c, dic_of_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#supplements for the expected h-index and avg h-index function that follows\n",
    "def another_suplement(m_list, m, field, dic_of_numPapers, cut_off):\n",
    "    tracker = 0\n",
    "    temp = []\n",
    "    for m, index in zip(sorted(m_list), range(1,len(m_list)+1)):\n",
    "        if dic_of_numPapers[field].count(m) > cut_off:\n",
    "            tracker += 1\n",
    "            \n",
    "        if tracker/index >= 0.95:\n",
    "            temp.append(m)\n",
    "    \n",
    "    return max(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def another_suplement2(m_list, m, field, dic_of_numPapers, cut_off):\n",
    "    tracker = 0\n",
    "    temp = []\n",
    "    for m, index in zip(sorted(m_list, reverse = True), range(1,len(m_list)+1)):\n",
    "        if dic_of_numPapers[field].count(m) < cut_off:\n",
    "            tracker += 1 \n",
    "        if tracker/index >= 0.95:\n",
    "            temp.append(m)\n",
    "    return min(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#constructs the confidence intervals for the avg h-index predicting expected value\n",
    "#as well as the cutoffs that define regimes with \"large\" and \"small\" m\n",
    "def confidence_int(fields, dic_of_numPapers, dic_of_hindex, dic_of_c, dic_of_a):\n",
    "    dic_of_ev_and_avg = {}\n",
    "    for field in fields:\n",
    "        \n",
    "        all_m = list(set(dic_of_numPapers[field]))\n",
    "        avg_h = []\n",
    "        EV = []\n",
    "        e_bars = []\n",
    "        m_for_err = []\n",
    "        avg_for_err = []\n",
    "        for m in all_m:\n",
    "            temp = 0\n",
    "            for k in range(1, m+1):\n",
    "                temp += p_k(dic_of_c[field], dic_of_a[field], m, k)\n",
    "            EV.append(temp)\n",
    "            \n",
    "            dof = len(dic_of_hindex[field][m])-1\n",
    "            if dof > 0:\n",
    "                sample_mean = np.mean(dic_of_hindex[field][m])\n",
    "                sample_var = stats.sem(dic_of_hindex[field][m])\n",
    "                left, right = stats.t.interval(0.95, dof, loc=sample_mean, scale=sample_var)\n",
    "                \n",
    "                e_bars.append([(right - sample_mean) , (sample_mean - left)])\n",
    "                m_for_err.append(m)\n",
    "                avg_for_err.append(sample_mean)\n",
    "            avg_h.append(sample_mean)\n",
    "        \n",
    "        cut_off1 = 25\n",
    "        cut_off2 = 10\n",
    "        result1 = another_suplement(all_m, m, field, dic_of_numPapers, cut_off1)\n",
    "        result2 = another_suplement2(all_m, m, field, dic_of_numPapers, cut_off2)\n",
    "        \n",
    "        e_bars = np.array(e_bars).T\n",
    "        dic_of_ev_and_avg[field] = {'m': all_m, 'ev': EV, 'avg': avg_h}\n",
    "        print(field)\n",
    "        lim1 = max(avg_h)\n",
    "        lim2 = max(EV)\n",
    "        fig, ax = plt.subplots()\n",
    "        \n",
    "        \n",
    "        ax.set_xlim(0, len(all_m))\n",
    "        ax.set_ylim(0, (max(lim1, lim2)+10))\n",
    "        #ms=10, mew=1\n",
    "        \n",
    "        ax.scatter(all_m, EV, color = 'red', label = 'Theoretical Expected h-index')\n",
    "        ax.scatter(all_m, avg_h, label ='arXiv Sample Average h-index')\n",
    "        ax.errorbar(m_for_err, avg_for_err, yerr = e_bars, fmt='none', linewidth = 0.5, label='95% Confidence Interval')\n",
    "\n",
    "        y = [i for i in range(int(max(lim1, lim2)+10))]\n",
    "        \n",
    "        x1 = [result1 for i in range(int(max(lim1, lim2)+10))]\n",
    "        x2 = [result2 for i in range(int(max(lim1, lim2)+10))]\n",
    "        ax.plot(x1, y, label = '95% of $m < {}$ have more than {} Authors'.format(result1, cut_off1))\n",
    "        ax.plot(x2, y, label = '95% of $m > {}$ have less than {} Authors'.format(result2, cut_off2))\n",
    "        ax.set_ylabel('h-index')\n",
    "        ax.set_xlabel('Number of Papers Published $(m)$')\n",
    "        ax.legend()\n",
    "        plt.savefig('e_val/'+ field + '_e_val')\n",
    "        #ax.set_aspect('equal', adjustable='box')\n",
    "        plt.show()\n",
    "    return dic_of_ev_and_avg\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.rcParams['figure.figsize'] = [12, 6]\n",
    "#dic_of_ev_and_avg = confidence_int(fields, dic_of_numPapers, dic_of_hindex, dic_of_c, dic_of_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is not used\n",
    "def error_for_m(fields, dic_of_ev_and_avg):\n",
    "    for field in fields:\n",
    "        print(field)\n",
    "        plt.plot(dic_of_ev_and_avg[field]['m'], np.abs(np.array(dic_of_ev_and_avg[field]['ev']) - np.array(dic_of_ev_and_avg[field]['avg'])))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#error_for_m(fields, dic_of_ev_and_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some further analysis using random distribution, rather than authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the \"synthetic\" authors and find the h-index distribution for these synthetic authors\n",
    "#function takes a fair amount of time to run for all the fields\n",
    "#you can choose the total number of authors \n",
    "#still use the proportional number of true m in every field\n",
    "def select_random_papers(fields, dic_by_field, dic_of_numPapers, prob_dic, total_num_authors, samp, verb = False):\n",
    "    dic_by_field_fake_h = {}\n",
    "    \n",
    "    pbar = progressbar.ProgressBar()\n",
    "    for field in pbar(fields):\n",
    "        dic_by_field_fake_h[field] = {}\n",
    "        j = 0\n",
    "        temp = samp[field]\n",
    "        paper_list = [paper.citations for paper in dic_by_field[field]]\n",
    "        for m in temp:\n",
    "            dic_by_field_fake_h[field][m] = []\n",
    "            if verb:\n",
    "                num_authors = int(len(dic_of_numPapers[field])*prob_dic[field][m])\n",
    "            else:\n",
    "                num_authors = int(total_num_authors*prob_dic[field][m])\n",
    "            j += 1\n",
    "            index = 1\n",
    "            for i in range(num_authors):\n",
    "                p_list = np.random.choice(paper_list, m)#sample\n",
    "                k = 0\n",
    "                p_list = sorted(p_list, reverse = True)\n",
    "                while p_list[k] >= k + 1 and k < len(p_list)-1:\n",
    "                    k += 1\n",
    "                dic_by_field_fake_h[field][m].append(k)\n",
    "\n",
    "    return dic_by_field_fake_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a small subset of fields if you want to use\n",
    "#small_sub_field = ['hep-th', 'hep-ex', 'hep-ph']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes a few hours\n",
    "#dic_by_field_fake_h = select_random_papers(fields, dic_by_field, dic_of_numPapers, prob_dic, 100000, samp, verb = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_for_given_m(fields, m_dist, dic_by_field_fake_h, fake = True, mean = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some modifications to our distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_k_mod(c, a, m, x, k): #find pk\n",
    "    return p_k_Beta(x, k, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(t, c, a): #need g(t) too\n",
    "    return a*(np.power(t, (-c/(c+a)))-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function creates new empirically inspired distribution for every author in the data base\n",
    "#it is based on their windowsize--meaning the window of indices of publications \n",
    "#then sample 100 times from this new empirical distribution\n",
    "#returns a h-index distribution\n",
    "def modified_dist_draw2(c, a, m, U_one, U_m, correct = True):\n",
    "    dist = [0.0]*(m+1)\n",
    "    if U_m != U_one:\n",
    "        h = [k for k in range(0, m+1)]\n",
    "        fac = 1/(U_m - U_one)\n",
    "        g_inv_list = [fac*(g_inverse(k, c, a)-U_one) for k in range(0, m+1)]\n",
    "        if correct:\n",
    "            correction = 0.5\n",
    "            g_inv_list1 = [fac*(g_inverse(k - correction, c, a)-U_one) for k in range(0, m+1)]\n",
    "            g_inv_list2 = [fac*(g_inverse(k + correction, c, a)-U_one) for k in range(0, m+1)]\n",
    "        else:\n",
    "            correction = 0.0\n",
    "        if g(U_one, c, a) < 1 - correction:\n",
    "            dist[0] = 1.0\n",
    "        elif g(U_m, c, a) >= m + correction:\n",
    "            dist[-1] = 1.0\n",
    "        else:\n",
    "            for k in h[1:-1]:\n",
    "                if k == 1:\n",
    "                    if correct:\n",
    "                        dist[k] = 1 -p_k_mod(c, a, m-2, g_inv_list1[k+1], k)\n",
    "                    else:\n",
    "                        dist[k] = 1 -p_k_mod(c, a, m-2, g_inv_list[k+1], k)\n",
    "\n",
    "                elif k == m-1:\n",
    "                    if correct:\n",
    "                        dist[k] = p_k_mod(c, a, m-2, g_inv_list2[k+1], k)\n",
    "                    else:\n",
    "                        dist[k] = p_k_mod(c, a, m-2, g_inv_list[k+1], k)\n",
    "                else:\n",
    "                    dist[k] = p_k_mod(c, a, m-2, g_inv_list[k+1], k) - p_k_mod(c, a, m-2, g_inv_list[k+2], k+1)\n",
    "        dist = np.nan_to_num(dist)\n",
    "        sum_ = np.sum(dist)\n",
    "        if sum_ < 1:\n",
    "            dist[0] = 1-sum_ #left Over mass\n",
    "        return np.random.choice(h, 100, p=dist)\n",
    "    else:\n",
    "        return np.array(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is the driver for finding the h-index distribution with our window size modification\n",
    "def driver(dic_of_c, dic_of_a, dic_of_numPapers, dic_by_field_authors, dic_by_field):\n",
    "    mod_hindex = {}\n",
    "    pbar = progressbar.ProgressBar()\n",
    "    for field in pbar(fields):\n",
    "        n = len(dic_by_field[field])\n",
    "        all_m = list(set(dic_of_numPapers[field]))\n",
    "        mod_hindex[field] = {}\n",
    "        for m in all_m:\n",
    "            mod_hindex[field][m] = []\n",
    "        for key,author in dic_by_field_authors[field].items():\n",
    "            if author.order_list2:\n",
    "                temp = sorted(author.order_list2)\n",
    "                m = len(author.order_list2)\n",
    "                if m > 1:\n",
    "                    mod_hindex[field][m].extend(list(modified_dist_draw2(dic_of_c[field], dic_of_a[field], m, temp[0], temp[-1], correct = False)))\n",
    "                elif m == 1:\n",
    "                    if g(temp[0], dic_of_c[field], dic_of_a[field]) >= 1:\n",
    "                        mod_hindex[field][m].extend([1]*100)\n",
    "                    else:\n",
    "                        mod_hindex[field][m].extend([0]*100)   \n",
    "                else:\n",
    "                    print('something is wrong')\n",
    "                    print(author.order_list2)\n",
    "    \n",
    "    return mod_hindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66% (14 of 21) |################        | Elapsed Time: 0:03:41 ETA:   0:00:19"
     ]
    }
   ],
   "source": [
    "mod_hindex = driver(dic_of_c, dic_of_a, dic_of_numPapers, dic_by_field_authors, dic_by_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plots the two histograms for our new modified distribution and the true arXiv citation networks\n",
    "def plot_for_given_m_two_histo(fields, m_dist1, m_dist2, sample = samp):\n",
    "    for field in fields:\n",
    "        temp = sample[field]\n",
    "        for m in temp:\n",
    "            fig, ax = plt.subplots()\n",
    "            print(field)\n",
    "            print(m)\n",
    "            bins_ = [i for i in range(max(list(set(dic_of_hindex[field][m])))+1)]\n",
    "            ax.hist((m_dist1[field][m], m_dist2[field][m]), bins=bins_, label = ['true', 'predicted'] , density = True, align = 'left')\n",
    "            ax.set_ylabel('$P(h-index = k | m = {})$'.format(m))\n",
    "            ax.set_xlabel('h-index')\n",
    "            ax.set_xticks(bins_)\n",
    "            plt.legend()\n",
    "            plt.savefig('empirical_dist_continuity/'+ field +'_'+str(m))\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plot_for_given_m_two_histo(fields, dic_of_hindex, mod_hindex)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
